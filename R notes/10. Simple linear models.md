## Simple LR
Technique to predict a continuous outcome variable, Y, on the basis of one predictor variable, X, where
> Y=beta0​+beta1​*X+error
- Where Y represents the continuous var
- Where X represents the predictor var
- Where beta_0 is the intercept, when X=0
- Where beta_1 is the coefficient of the line representing the relationship

To do this in R, we use `lm()`:
```r
model <- lm(outcome ~ indep_var, data = dataset)
```

### Train and test split
Where a certain amount of data is used to train the model, and the remaining to increase the model's accuracy and generalizability.  
Data can be randomly assigned to test/training using the `sample()` function in the `conversion_clean` data set:
```r
# specify 60/40 split, create data_sample variable
data_sample <- sample(c(TRUE, FALSE),
                      nrow(conversion_clean),
                      replace = TRUE,
                      prob = c(0.6, 0.4))
# subset into train and test using list (row) indexing
train <- conversion_clean[data_sample, ]   # rows where data_sample is TRUE
test  <- conversion_clean[!data_sample, ]  # rows where data_sample is FALSE
```

We can then fit this to a linear regression model with `lm()`:
```r
model <- lm(outcome ~ indep_var,
data = train)
```

### Quantifying model fit (Goodness of Fit)
1. **Residual standard error**: Estimate of SD of error, by `summary(model)` or `sigma(model)`
2. **R squared**: Proportion of variance explained, by `summary(model)` or `summary(model)$r.squared`
   - e.g. `advertising_budget` explain 64% of the variability in the total `sales` value.
