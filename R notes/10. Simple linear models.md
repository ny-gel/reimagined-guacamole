## Simple LR
Technique to predict a continuous outcome variable, Y, on the basis of one predictor variable, X, where
> Y=beta0​+beta1​*X+error
- Where Y represents the continuous var
- Where X represents the predictor var
- Where beta_0 is the intercept, when X=0
- Where beta_1 is the coefficient of the line representing the relationship

To do this in R, we use `lm()`:
```r
model <- lm(outcome ~ indep_var, data = dataset)
```

### Train and test split
Where a certain amount of data is used to train the model, and the remaining to increase the model's accuracy and generalizability.  
Data can be randomly assigned to test/training using the `sample()` function in the `conversion_clean` data set:
```r
# specify 60/40 split, create data_sample variable
data_sample <- sample(c(TRUE, FALSE),
                      nrow(conversion_clean),
                      replace = TRUE,
                      prob = c(0.6, 0.4))
# subset into train and test using list (row) indexing
train <- conversion_clean[data_sample, ]   # rows where data_sample is TRUE
test  <- conversion_clean[!data_sample, ]  # rows where data_sample is FALSE
```

We can then fit this to a linear regression model with `lm()`:
```r
model <- lm(outcome ~ indep_var,
data = train)
```

### Quantifying model fit (Goodness of Fit)
1. **Residual standard error**: Estimate of SD of error, by `summary(model)` or `sigma(model)`
2. **R squared**: Proportion of variance explained, by `summary(model)` or `summary(model)$r.squared`
   - e.g. `advertising_budget` explain 64% of the variability in the total `sales` value.

Using the example from codecademy:
```r
# compute r-squared for both models
r_sq <- summary(model)$r.squared 
r_sq_2 <- summary(model_2)$r.squared
```
Interpretation:
- Based on a pair of simple linear regression models, we have determined that 62.8% of the variation in user purchase behavior can be explained by the number of times a user viewed on a relevant ad campaign;
- Whereas only 43.3% of this variation can be explained by the number of times a user clicked on a relevant ad.

3. **Residual calculation**: Vertical distance between a datapoint and the line estimated by a regression model
To obtain the points which form the (1) estimate line, and (2) residual values, use the `predict()` and `residuals()` functions.  

For example, in the `train` dataset:
```r
#save predicted and residual values to df
train$estimate <- predict(model) # saved to column named estimate
train$residuals <- residuals(model) # saved to column named residuals
```

To plot the observed data points from the `train` dataset, where y = `estimate`:
```r
#create visualization
plot <- ggplot(data = train, aes(x=clicks, y=total_convert)) +
geom_point(aes(size = abs(residuals))) + # observe size of residuals
geom_point(aes(y=estimate),color = "blue") +
geom_segment(aes(xend = clicks, yend = estimate), color = "gray") # add vertical distance between observed points and estimate line
```
